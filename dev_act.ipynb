{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"try to use nbdev @jeremyhoward to write scripts\"\"\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.Tensor at 0x215040a0e48>,\n",
       " <__main__.Tensor at 0x215040a0f08>,\n",
       " <__main__.Tensor at 0x215040a0e48>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib import layer_init\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self,child=None,h=1,w=1,weight=None,trainable=True,grad=None):\n",
    "        if weight is None:\n",
    "            weight = layer_init(h,w)\n",
    "        self.weight = weight\n",
    "        self.trainable = trainable\n",
    "        self.grad = grad\n",
    "        self.child = child\n",
    "\n",
    "    def __call__(self,child):\n",
    "        self.child = child\n",
    "\n",
    "a = Tensor(weight=1.)\n",
    "b = Tensor()\n",
    "b(a)\n",
    "a, b, b.child, a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Act(Tensor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self,bpass):\n",
    "        if self.child is not None:\n",
    "            # backprop without assigning variable explicitly\n",
    "            self.child.backward(np.multiply(self.grad,bpass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [-1.  -0.5  0.   0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0. , 0. , 0. , 0.5]), array([0., 0., 0., 1.], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class relu(Act):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = np.maximum(x,0)\n",
    "        self.grad = (out > 0).astype(np.float32)\n",
    "        return out\n",
    "\n",
    "x = np.arange(-1.,1.,0.5)\n",
    "print(\"x: \",x)\n",
    "xp = relu()\n",
    "xp.forward(x), xp.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.26894142, 0.37754067, 0.5       , 0.62245933]),\n",
       " array([0.19661193, 0.23500371, 0.25      , 0.23500371]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class sigmoid(Act):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        S = np.array(list(map(lambda x: 1/(1+np.exp(-x)), x)))\n",
    "        self.grad = np.multiply(S, (1-S))\n",
    "        return S\n",
    "\n",
    "xpp = sigmoid()\n",
    "xpp.forward(x), xpp.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86561971]] True None\n"
     ]
    }
   ],
   "source": [
    "class Layer(Tensor):\n",
    "    def __init__(self,child=None):\n",
    "        super().__init__()\n",
    "        self.fpass = None\n",
    "        # can refactor it as list\n",
    "        self.child = child \n",
    "\n",
    "    def forward(self,x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self,grad):\n",
    "        raise NotImplementedError\n",
    "\n",
    "c = Layer()\n",
    "d = Layer()\n",
    "d(c)\n",
    "assert c == d.child\n",
    "print(c.weight, c.trainable, c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.4330174]]),\n",
       " None,\n",
       " True,\n",
       " None,\n",
       " array([[0.2165087]]),\n",
       " None,\n",
       " array([[0.05]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.fpass = x\n",
    "        return x @ self.weight\n",
    "\n",
    "    def backward(self,bpass):\n",
    "        self.grad = self.fpass.T @ bpass\n",
    "        # for backprop, not used yet\n",
    "        # \"child\" not a list yet\n",
    "        if self.child is not None:\n",
    "            self.child.backward(bpass @ (self.weight.T))\n",
    "\n",
    "a = Linear()\n",
    "x = np.array([[0.5]])\n",
    "xg = np.array([[0.1]])\n",
    "a.weight,a.child, a.trainable, a.grad, a.forward(x), a.backward(xg), a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if we declare:\n",
    "    fn = Loss().mse\n",
    "\n",
    "can we visited the Loss() object of \"fn\"\n",
    " to call \"backward()\" directly\n",
    "\"\"\"\n",
    "\n",
    "class Loss:\n",
    "    \"\"\"Take care of dimension problem (batch, input-D)\"\"\"\n",
    "    def __init__(self,last_layer=None):\n",
    "        self.grad = None\n",
    "        self.child = last_layer\n",
    "    \n",
    "    def mse(self,y,yhat):\n",
    "        loss = np.square(np.subtract(y,yhat))\n",
    "        grad = np.multiply(2.,np.subtract(y,yhat))\n",
    "        # for backprop\n",
    "        self.grad = grad\n",
    "\n",
    "        return loss.sum()\n",
    "        # grad is negligible since we \n",
    "        #  have saved it in .grad\n",
    "        #return loss.sum(), grad.mean()\n",
    "\n",
    "    def backward(self):\n",
    "        if self.grad is None:\n",
    "            print(\"loss function hasn't been called yet\")\n",
    "            raise NotImplementedError\n",
    "        if self.child is not None:\n",
    "            self.child.backward(self.grad)\n",
    "\n",
    "xx = np.array([0,1,2,3,4])\n",
    "yy = xx + 1\n",
    "Loss().mse(xx,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35782659]] None\n",
      "[[0.17891329]]\n",
      "0.385748695986333\n",
      "[[0.35782659]] [[0.62108671]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.24217341]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target\n",
    "xt = np.array([[0.8]])\n",
    "\n",
    "# establish layer\n",
    "a = Linear()\n",
    "loss = Loss(last_layer=a)\n",
    "print(a.weight, a.grad)\n",
    "out = a.forward(x)\n",
    "print(out)\n",
    "print(loss.mse(xt,out))\n",
    "loss.backward()\n",
    "print(a.weight, a.grad)\n",
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35782659]] [[0.62108671]]\n",
      "[[0.35776448]]\n"
     ]
    }
   ],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self,model=[],lr=1e-4):\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "    \n",
    "    # now, sequential only\n",
    "    def sgd(self):\n",
    "        for layer in self.model:\n",
    "            layer.weight -= self.lr * layer.grad\n",
    "\n",
    "\n",
    "print(a.weight, a.grad)\n",
    "optim = Optimizer([a]).sgd\n",
    "optim()\n",
    "print(a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1aed05da3f539dc249e83c9fc673e6c9bbdc93d09eb5d382f4a29d54335c6235"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
