{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"try to use nbdev @jeremyhoward to write scripts\"\"\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.Tensor at 0x1b9fd156148>,\n",
       " <__main__.Tensor at 0x1b9fd156608>,\n",
       " <__main__.Tensor at 0x1b9fd156148>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib import layer_init\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self,child=None,h=1,w=1,weight=None,trainable=True,grad=None):\n",
    "        if weight is None:\n",
    "            weight = layer_init(h,w)\n",
    "        self.weight = weight\n",
    "        self.trainable = trainable\n",
    "        self.grad = grad\n",
    "        self.child = child\n",
    "\n",
    "    def __call__(self,child):\n",
    "        self.child = child\n",
    "\n",
    "a = Tensor(weight=1.)\n",
    "b = Tensor()\n",
    "b(a)\n",
    "a, b, b.child, a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Act(Tensor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self,bpass):\n",
    "        if self.child is not None:\n",
    "            # backprop without assigning variable explicitly\n",
    "            self.child.backward(np.multiply(self.grad,bpass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [-1.  -0.5  0.   0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0. , 0. , 0. , 0.5]), array([0., 0., 0., 1.], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class relu(Act):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = np.maximum(x,0)\n",
    "        self.grad = (out > 0).astype(np.float32)\n",
    "        return out\n",
    "\n",
    "x = np.arange(-1.,1.,0.5)\n",
    "print(\"x: \",x)\n",
    "xp = relu()\n",
    "xp.forward(x), xp.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.26894142, 0.37754067, 0.5       , 0.62245933]),\n",
       " array([0.19661193, 0.23500371, 0.25      , 0.23500371]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class sigmoid(Act):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        S = np.array(list(map(lambda x: 1/(1+np.exp(-x)), x)))\n",
    "        self.grad = np.multiply(S, (1-S))\n",
    "        return S\n",
    "\n",
    "xpp = sigmoid()\n",
    "xpp.forward(x), xpp.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32916021]] True None\n"
     ]
    }
   ],
   "source": [
    "class Layer(Tensor):\n",
    "    def __init__(self,child=None):\n",
    "        super().__init__()\n",
    "        self.fpass = None\n",
    "        # can refactor it as list\n",
    "        self.child = child \n",
    "\n",
    "    def forward(self,x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self,grad):\n",
    "        raise NotImplementedError\n",
    "\n",
    "c = Layer()\n",
    "d = Layer()\n",
    "d(c)\n",
    "assert c == d.child\n",
    "print(c.weight, c.trainable, c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.28574475]]),\n",
       " None,\n",
       " True,\n",
       " None,\n",
       " array([[-0.14287237]]),\n",
       " None,\n",
       " array([[0.05]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.fpass = x\n",
    "        return x @ self.weight\n",
    "\n",
    "    def backward(self,bpass):\n",
    "        self.grad = self.fpass.T @ bpass\n",
    "        # for backprop, not used yet\n",
    "        # \"child\" not a list yet\n",
    "        if self.child is not None:\n",
    "            self.child.backward(bpass @ (self.weight.T))\n",
    "\n",
    "a = Linear()\n",
    "x = np.array([[0.5]])\n",
    "xg = np.array([[0.1]])\n",
    "a.weight,a.child, a.trainable, a.grad, a.forward(x), a.backward(xg), a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if we declare:\n",
    "    fn = Loss().mse\n",
    "\n",
    "can we visited the Loss() object of \"fn\"\n",
    " to call \"backward()\" directly\n",
    "\"\"\"\n",
    "\n",
    "class Loss:\n",
    "    \"\"\"Take care of dimension problem (batch, input-D)\"\"\"\n",
    "    def __init__(self,last_layer=None):\n",
    "        self.grad = None\n",
    "        self.child = last_layer\n",
    "    \n",
    "    def mse(self,y,yhat):\n",
    "        loss = np.square(np.subtract(y,yhat))\n",
    "        grad = np.multiply(2.,np.subtract(y,yhat))\n",
    "        # for backprop\n",
    "        self.grad = grad\n",
    "\n",
    "        return loss.sum()\n",
    "        # grad is negligible since we \n",
    "        #  have saved it in .grad\n",
    "        #return loss.sum(), grad.mean()\n",
    "\n",
    "    def backward(self):\n",
    "        if self.grad is None:\n",
    "            print(\"loss function hasn't been called yet\")\n",
    "            raise NotImplementedError\n",
    "        if self.child is not None:\n",
    "            self.child.backward(self.grad)\n",
    "\n",
    "xx = np.array([0,1,2,3,4])\n",
    "yy = xx + 1\n",
    "Loss().mse(xx,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96709294]] None\n",
      "[[0.48354647]]\n",
      "0.10014283745376988\n",
      "[[0.96709294]] [[0.31645353]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.63290706]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target\n",
    "xt = np.array([[0.8]])\n",
    "\n",
    "# establish layer\n",
    "a = Linear()\n",
    "loss = Loss(last_layer=a)\n",
    "print(a.weight, a.grad)\n",
    "out = a.forward(x)\n",
    "print(out)\n",
    "print(loss.mse(xt,out))\n",
    "loss.backward()\n",
    "print(a.weight, a.grad)\n",
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96709294]] [[0.31645353]]\n",
      "[[0.96706129]]\n"
     ]
    }
   ],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self,model=[],lr=1e-4):\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "    \n",
    "    # now, sequential only\n",
    "    def sgd(self):\n",
    "        for layer in self.model:\n",
    "            layer.weight -= self.lr * layer.grad\n",
    "\n",
    "\n",
    "print(a.weight, a.grad)\n",
    "optim = Optimizer([a]).sgd\n",
    "optim()\n",
    "print(a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.array([[1],[2],[3],[4]])\n",
    "label = train+0.1\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0, loss: 0.01767\n",
      "epoch:   1, loss: 0.02750\n",
      "epoch:   2, loss: 0.03942\n",
      "epoch:   3, loss: 0.05332\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):\n",
    "    for i in range(4):\n",
    "        out = a.forward(train[i])\n",
    "        loss_val = loss.mse(out,label[i])\n",
    "        loss.backward()\n",
    "        optim()\n",
    "\n",
    "        losses.append(loss_val.mean())\n",
    "\n",
    "for i, val in enumerate(losses):\n",
    "    print(\"epoch: %3d, loss: %.5f\" %(i,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1), array([[0.96745806]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight.shape, a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    x = Linear(h=1,w=2)\n",
    "    x = Linear(child=x,h=2,w=1)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-3f041579562e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-e555dd62bb70>\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'h'"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "loss = Loss(model).mse\n",
    "optims = Optimizer([model]).sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1aed05da3f539dc249e83c9fc673e6c9bbdc93d09eb5d382f4a29d54335c6235"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
